---
phase: 42-sync-layer
plan: 02
type: execute
wave: 2
depends_on: ["42-01"]
files_modified:
  - packages/mcp-server/src/sync/block-sync.ts
  - packages/mcp-server/src/sync/index.ts
autonomous: true

must_haves:
  truths:
    - "New blocks (folder exists, not in metadata) are detected and synced"
    - "Changed blocks (hash differs from stored hash) are re-synced"
    - "Deleted blocks (in metadata, folder gone) have data removed from DuckDB"
    - "Sync failures roll back cleanly with no partial state"
    - "Corrupt CSV files are skipped, other blocks continue syncing"
  artifacts:
    - path: "packages/mcp-server/src/sync/block-sync.ts"
      provides: "Block sync logic"
      exports: ["syncBlockInternal", "detectBlockChanges", "cleanupDeletedBlocks"]
    - path: "packages/mcp-server/src/sync/index.ts"
      provides: "Public sync API"
      exports: ["syncAllBlocks", "syncBlock"]
  key_links:
    - from: "packages/mcp-server/src/sync/block-sync.ts"
      to: "packages/mcp-server/src/db/connection.ts"
      via: "getConnection for DB operations"
      pattern: "getConnection\\(baseDir\\)"
    - from: "packages/mcp-server/src/sync/block-sync.ts"
      to: "packages/mcp-server/src/sync/hasher.ts"
      via: "hashFileContent for change detection"
      pattern: "hashFileContent"
    - from: "packages/mcp-server/src/sync/block-sync.ts"
      to: "packages/mcp-server/src/sync/metadata.ts"
      via: "sync metadata CRUD"
      pattern: "(get|upsert|delete)SyncMetadata"
---

<objective>
Implement block CSV to DuckDB synchronization with hash-based change detection, atomic transactions, and error handling.

Purpose: Satisfy SYNC-02 (new blocks), SYNC-03 (changed blocks), SYNC-04 (deleted blocks), SYNC-05 (transaction-safe), SYNC-06 (lazy sync).
Output: Working `syncAllBlocks()` and `syncBlock()` functions that keep DuckDB trade data in sync with CSV files.
</objective>

<execution_context>
@/Users/davidromeo/.claude/get-shit-done/workflows/execute-plan.md
@/Users/davidromeo/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/42-sync-layer/42-CONTEXT.md
@.planning/phases/42-sync-layer/42-RESEARCH.md
@.planning/phases/42-sync-layer/42-01-SUMMARY.md

# Existing code to reference
@packages/mcp-server/src/utils/block-loader.ts
@packages/mcp-server/src/db/connection.ts
@packages/mcp-server/src/sync/index.ts
@packages/mcp-server/src/sync/metadata.ts
@packages/mcp-server/src/sync/hasher.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create block sync logic</name>
  <files>
    packages/mcp-server/src/sync/block-sync.ts
  </files>
  <action>
Create `packages/mcp-server/src/sync/block-sync.ts` with:

1. **Import dependencies:**
   - `getConnection` from db/connection.js
   - `hashFileContent` from ./hasher.js
   - `getSyncMetadata`, `upsertSyncMetadata`, `deleteSyncMetadata`, `getAllSyncedBlockIds` from ./metadata.js
   - `BlockSyncMetadata` type from ./metadata.js
   - `fs/promises` for file operations
   - `path` for path manipulation

2. **Types:**
   ```typescript
   export interface BlockSyncResult {
     blockId: string;
     status: 'synced' | 'unchanged' | 'deleted' | 'error';
     tradeCount?: number;
     error?: string;
   }
   ```

3. **`syncBlockInternal(conn, blockId, blockPath): Promise<BlockSyncResult>`**
   Core sync logic for a single block:
   - Find tradelog CSV (check block.json csvMappings first, then standard "tradelog.csv", then CSV discovery)
   - Hash the tradelog file with `hashFileContent()`
   - Get existing metadata with `getSyncMetadata(conn, blockId)`
   - If hash matches, return `{ status: 'unchanged' }`
   - If hash differs or no metadata exists:
     a. `BEGIN TRANSACTION`
     b. `DELETE FROM trades.trade_data WHERE block_id = ?`
     c. Parse CSV using existing `parseCSV` pattern from block-loader.ts (reuse or import)
     d. INSERT trades in batches of 500 rows (multi-value INSERT for efficiency)
     e. Hash dailylog and reportinglog if they exist
     f. `upsertSyncMetadata()` with new hashes
     g. `COMMIT`
     h. Return `{ status: 'synced', tradeCount }`
   - On error: `ROLLBACK`, return `{ status: 'error', error: message }`

4. **`detectBlockChanges(conn, baseDir): Promise<{ toSync: string[], toDelete: string[] }>`**
   Compare filesystem vs metadata:
   - List all directories in baseDir (excluding hidden, excluding _marketdata)
   - Get all block_ids from `getAllSyncedBlockIds(conn)`
   - `toSync`: folders that exist but either:
     - Not in metadata (new block)
     - OR have different tradelog hash (changed block - requires reading file and hashing)
   - `toDelete`: block_ids in metadata but folder doesn't exist

5. **`cleanupDeletedBlocks(conn, deletedBlockIds): Promise<void>`**
   For each deleted blockId:
   - `BEGIN TRANSACTION`
   - `DELETE FROM trades.trade_data WHERE block_id = ?`
   - `deleteSyncMetadata(conn, blockId)`
   - `COMMIT`

6. **Helper: `parseCsvFile(filePath): Promise<Record<string, string>[]>`**
   Reuse the parseCSV + parseCSVLine logic from block-loader.ts. Either:
   - Import if block-loader exports it, OR
   - Copy the parseCSV and parseCSVLine functions locally (they're small)

7. **Helper: `insertTradeBatch(conn, blockId, trades, startIdx, batchSize): Promise<void>`**
   Build multi-row INSERT statement:
   ```sql
   INSERT INTO trades.trade_data (block_id, date_opened, time_opened, strategy, ...) VALUES
     (?, ?, ?, ?, ...),
     (?, ?, ?, ?, ...),
     ...
   ```
   Use parameter binding to avoid SQL injection. DuckDB supports positional params.

**Per CONTEXT.md decisions:**
- Use hash for change detection (NOT mtime)
- Skip failed blocks, continue others
- Atomic transactions (DELETE + INSERT in single transaction)
- If sync fails for a previously-synced block, REMOVE its data from DuckDB (no stale data)
  </action>
  <verify>
- File exists at sync/block-sync.ts
- `npm run build` succeeds in packages/mcp-server
- Exports: `syncBlockInternal`, `detectBlockChanges`, `cleanupDeletedBlocks`
  </verify>
  <done>Block sync logic implemented with hash-based detection and transaction safety.</done>
</task>

<task type="auto">
  <name>Task 2: Implement syncAllBlocks and syncBlock public API</name>
  <files>
    packages/mcp-server/src/sync/index.ts
  </files>
  <action>
Update `packages/mcp-server/src/sync/index.ts` to implement the placeholder functions:

1. **Update imports:**
   ```typescript
   import { getConnection } from "../db/connection.js";
   import { syncBlockInternal, detectBlockChanges, cleanupDeletedBlocks } from "./block-sync.js";
   ```

2. **`syncAllBlocks(baseDir: string): Promise<SyncResult>`**
   Full sync triggered by `list_blocks`:
   ```typescript
   export async function syncAllBlocks(baseDir: string): Promise<SyncResult> {
     const conn = await getConnection(baseDir);
     const results: BlockSyncResult[] = [];
     const errors: string[] = [];

     // 1. Detect changes
     const { toSync, toDelete } = await detectBlockChanges(conn, baseDir);

     // 2. Delete orphaned blocks
     for (const blockId of toDelete) {
       try {
         await cleanupDeletedBlocks(conn, [blockId]);
         results.push({ blockId, status: 'deleted' });
       } catch (err) {
         errors.push(`Failed to delete ${blockId}: ${err}`);
       }
     }

     // 3. Sync changed/new blocks
     for (const blockId of toSync) {
       const blockPath = path.join(baseDir, blockId);
       const result = await syncBlockInternal(conn, blockId, blockPath);
       results.push(result);
       if (result.status === 'error' && result.error) {
         errors.push(`${blockId}: ${result.error}`);
       }
     }

     return {
       blocksProcessed: results.length,
       synced: results.filter(r => r.status === 'synced').length,
       unchanged: results.filter(r => r.status === 'unchanged').length,
       deleted: results.filter(r => r.status === 'deleted').length,
       errors
     };
   }
   ```

3. **`syncBlock(blockId: string, baseDir: string): Promise<BlockSyncResult>`**
   Single-block sync for per-tool use:
   ```typescript
   export async function syncBlock(blockId: string, baseDir: string): Promise<BlockSyncResult> {
     const conn = await getConnection(baseDir);
     const blockPath = path.join(baseDir, blockId);

     // Check if folder exists
     try {
       await fs.access(blockPath);
     } catch {
       // Block folder doesn't exist - if it was synced before, clean it up
       const existing = await getSyncMetadata(conn, blockId);
       if (existing) {
         await cleanupDeletedBlocks(conn, [blockId]);
         return { blockId, status: 'deleted' };
       }
       return { blockId, status: 'error', error: `Block folder not found: ${blockId}` };
     }

     return syncBlockInternal(conn, blockId, blockPath);
   }
   ```

4. **Update SyncResult type** if not already defined:
   ```typescript
   export interface SyncResult {
     blocksProcessed: number;
     synced: number;
     unchanged: number;
     deleted: number;
     errors: string[];
   }
   ```

5. **Remove the placeholder `throw new Error('Not implemented')` lines** from syncAllBlocks and syncBlock.

6. **Keep syncMarketData as placeholder** (will be implemented in Plan 03).
  </action>
  <verify>
- `npm run build` succeeds
- `syncAllBlocks` and `syncBlock` no longer throw "Not implemented"
- Can import and call the functions (they execute, though may return empty results)
  </verify>
  <done>Public sync API implemented and ready for tool integration.</done>
</task>

</tasks>

<verification>
1. `npm run build` succeeds in packages/mcp-server
2. Block sync logic handles:
   - New blocks (folder exists, not in metadata)
   - Changed blocks (hash differs)
   - Deleted blocks (folder gone, was in metadata)
   - Errors (corrupt CSV skipped, others continue)
3. Transactions are atomic (tested by reviewing code for BEGIN/COMMIT/ROLLBACK pattern)
4. Hash-based change detection (no mtime dependencies)
</verification>

<success_criteria>
- syncAllBlocks() detects and syncs all blocks, reports errors
- syncBlock() syncs single block on demand
- Deleted blocks have data removed from DuckDB
- Corrupt CSVs are skipped with error reported, other blocks sync successfully
- No partial state in database (transactions rollback on failure)
</success_criteria>

<output>
After completion, create `.planning/phases/42-sync-layer/42-02-SUMMARY.md`
</output>
