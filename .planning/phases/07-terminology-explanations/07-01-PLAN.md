---
phase: 07-terminology-explanations
plan: 01
type: execute
---

<objective>
Add clear, helpful terminology explanations throughout the WFA UI so newcomers understand what they're looking at.

Purpose: Users new to walk-forward analysis need to understand IS/OOS, robustness metrics, and window concepts to interpret results meaningfully. This phase addresses ISS-002 (confusing Avg Performance Delta) and ensures complete tooltip coverage.

Output: Enhanced tooltips across WFA components with deeper, genuinely helpful explanations.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-terminology-explanations/07-CONTEXT.md

# Key files to modify:
@components/walk-forward/robustness-metrics.tsx
@components/walk-forward/walk-forward-summary.tsx
@app/(platform)/walk-forward/page.tsx

# Reference for existing tooltip patterns:
@components/metric-card.tsx
@components/walk-forward/period-selector.tsx

**Accumulated decisions:**
- Phase 6: Efficiency shown in Summary (intuitive "is it overfit?"), Robustness Score in Details (for comparing runs)
- Phase 6: Avg Performance Delta explanation deferred to this phase (ISS-002)

**Context from 07-CONTEXT.md:**
- IS/OOS clarity is THE core concept everything depends on
- Depth over breadth: explanations should be genuinely helpful, not just definitions
- Out of scope: interpretation guidance ("is this good or bad?") belongs in Phase 8
</context>

<tasks>

<task type="auto">
  <name>Task 1: Enhance robustness metric tooltips</name>
  <files>components/walk-forward/robustness-metrics.tsx</files>
  <action>
Improve tooltip content for all metrics in RobustnessMetrics component:

1. **Avg Performance Delta** (ISS-002 - priority):
   - Current: "Average percentage change between in-sample and out-of-sample performance"
   - Problem: Users don't understand why this matters or what values are good
   - New flavor: "How much performance dropped when tested on new data"
   - New detailed: "This shows the gap between optimization results and real-world testing. A value near 0% means your strategy performs similarly on new data as it did during training. Negative values (like -15%) mean out-of-sample performance was 15% worse. Large negative drops (beyond -20%) often indicate overfitting—the strategy memorized past patterns that don't repeat."

2. **Efficiency Ratio**:
   - Current is decent but clarify relationship to overfitting
   - New flavor: "How much of your optimized performance survived real-world testing"
   - New detailed: "If you achieved $1000 during optimization and $800 on new data, efficiency is 80%. Values above 70% suggest a robust strategy. Below 50% is a red flag—your strategy may be overfit to historical quirks that won't repeat."

3. **Robustness Score**:
   - Clarify it's a composite for comparing runs, not a standalone verdict
   - New flavor: "A combined quality score for comparing different analysis runs"
   - New detailed: "Blends efficiency, parameter stability, and consistency into one number. Useful for quickly comparing runs with different settings—higher is better. Don't fixate on the absolute number; use it to see if changes improved or hurt overall robustness."

4. **Consistency Score**:
   - Make it clearer this is about window-by-window performance
   - New flavor: "How often your strategy stayed profitable across different time periods"
   - New detailed: "If you tested 10 windows and 7 were profitable out-of-sample, consistency is 70%. High consistency (60%+) suggests your strategy adapts well to different market conditions. Low consistency means performance varies wildly—some periods win big, others lose."

5. **Parameter Stability**:
   - Current is good, minor enhancement
   - New flavor: "Whether the 'best' settings stayed similar across different time periods"
   - New detailed: "If optimal parameters swing wildly (e.g., Kelly 0.3 one window, 1.5 the next), the strategy may be unstable. High stability (70%+) means you can use a single set of parameters with confidence."
  </action>
  <verify>Read the file and confirm all 5 metrics have enhanced tooltip content with both flavor and detailed text</verify>
  <done>All robustness metric tooltips enhanced with clearer, more actionable explanations. Avg Performance Delta specifically addresses ISS-002.</done>
</task>

<task type="auto">
  <name>Task 2: Add IS/OOS foundational explanation to Summary</name>
  <files>components/walk-forward/walk-forward-summary.tsx</files>
  <action>
Add a prominent IS/OOS explanation to the WalkForwardSummary component since this is THE foundational concept:

1. Add an info icon next to the main headline ("Looking Good" / "Mixed Results" / "Needs Attention") that opens a HoverCard explaining the core IS/OOS concept:
   - Title: "What Walk-Forward Analysis Tests"
   - Flavor: "Did your strategy work on data it never saw during optimization?"
   - Detailed: "Walk-forward analysis splits your trading history into training windows (in-sample) and testing windows (out-of-sample). During training, the optimizer finds the best parameters. Those parameters are then tested on the next chunk of unseen data—simulating what happens when you trade live with optimized settings. If performance holds up on unseen data, your strategy is robust. If it collapses, you may have overfit to historical noise."

2. Also enhance the three summary metric card tooltips (Efficiency, Stability, Consistency) to reference IS/OOS explicitly:
   - Efficiency tooltip: "Efficiency compares out-of-sample performance to in-sample. High efficiency means your optimized settings worked well on new data."
   - Stability tooltip: "Stability measures how much the optimal parameters changed across different time periods. Stable parameters suggest a consistent strategy."
   - Consistency tooltip: "Consistency shows what fraction of out-of-sample windows were profitable. Higher is better—it means your strategy worked across different market conditions."

Import HelpCircle from lucide-react if not already imported. Follow the existing HoverCard pattern used in MetricCard.
  </action>
  <verify>Read the file and confirm: (1) info icon with HoverCard exists near headline, (2) three summary metric tooltips are enhanced with IS/OOS context</verify>
  <done>WalkForwardSummary has foundational IS/OOS explanation accessible from headline, and all three summary metrics have enhanced tooltips with IS/OOS context.</done>
</task>

<task type="auto">
  <name>Task 3: Enhance "How it works" dialog with terminology depth</name>
  <files>app/(platform)/walk-forward/page.tsx</files>
  <action>
Enhance the existing "How it works" dialog to serve as a terminology glossary:

1. Restructure the dialog content into clear sections:

   **What is Walk-Forward Analysis?**
   Walk-forward analysis tests whether your optimized strategy settings work on data they've never seen. It repeatedly:
   1. Optimizes on a training window (in-sample)
   2. Tests those settings on the next chunk of unseen data (out-of-sample)
   3. Moves forward in time and repeats

   **Key Terms:**
   - **In-Sample (IS)**: The historical period used to find optimal parameters. Think of it as the "training data."
   - **Out-of-Sample (OOS)**: The forward period used to test those parameters. Think of it as "final exam data" the optimizer never saw.
   - **Efficiency**: How much of your in-sample performance survived out-of-sample testing. 80% efficiency = 80% of gains held up.
   - **Robustness**: Whether your strategy performs consistently across different time periods, not just one lucky stretch.

   **What Good Results Look Like:**
   - Efficiency above 70%: Your optimized settings transfer well to new data
   - Consistency above 60%: Most windows were profitable out-of-sample
   - Stable parameters: The "best" settings didn't swing wildly between windows

   **Warning Signs:**
   - Efficiency below 50%: Settings that worked in training failed on new data
   - Low consistency: Performance varies wildly between windows
   - Unstable parameters: Optimal settings change dramatically each period

2. Keep the existing tips list at the bottom but reformat them to integrate with the new structure.

Note: The actual "is this result good or bad?" guidance is Phase 8 territory. This task focuses on helping users understand WHAT the terms mean, not WHETHER their specific results are good.
  </action>
  <verify>Open the dialog in the UI and confirm the enhanced terminology content is present with clear sections</verify>
  <done>"How it works" dialog enhanced with structured terminology glossary covering IS/OOS, efficiency, robustness, and warning signs at a conceptual level.</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `npm run build` succeeds without errors
- [ ] All robustness metric tooltips enhanced (5 metrics in robustness-metrics.tsx)
- [ ] WalkForwardSummary has IS/OOS foundational explanation near headline
- [ ] "How it works" dialog has structured terminology sections
- [ ] ISS-002 (Avg Performance Delta confusion) addressed with clear explanation
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No TypeScript errors
- Every WFA-specific term has a helpful tooltip
- IS/OOS concept is prominently explained for newcomers
- Avg Performance Delta (ISS-002) has clear, actionable explanation
</success_criteria>

<output>
After completion, create `.planning/phases/07-terminology-explanations/07-01-SUMMARY.md`:

# Phase 7 Plan 01: Terminology Explanations Summary

**[Substantive one-liner]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `path/to/file.ts` - Description

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Step

Phase complete, ready for Phase 8 (Interpretation Guidance)
</output>
